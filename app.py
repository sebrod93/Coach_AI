import cv2
import matplotlib.pyplot as plt
import mediapipe as mp
import numpy as np
import pandas as pd
import pickle
import requests
import streamlit as st
import tempfile
from Coach_AI.data import json_to_df
from Coach_AI.exercices import exercices
from Coach_AI.utils import make_plot, download_model
from streamlit_webrtc import VideoTransformerBase, webrtc_streamer

emojis = {
    'cardio': 'ðŸ«€',
    'pecs and arms': 'ðŸ’ªðŸ»',
    'thighs': 'ðŸ¦µðŸ»',
    'glutes and calfs': 'ðŸ‘',
    'back muscles': 'ðŸ”™'
}


st.title('Coach AI')


#Instantiate Mediapipe Objects
mp_drawing = mp.solutions.drawing_utils
mp_drawing_styles = mp.solutions.drawing_styles
mp_pose = mp.solutions.pose


app_mode = st.sidebar.selectbox('Choose the App mode',
                                ['Run on Video', 'Run on Webcam'])

if app_mode == 'Run on Video':
    st.sidebar.header('Upload your video :video_camera:')

    video = st.sidebar.file_uploader('Video File',
                                 type=["mp4", "mov", 'avi', 'asf', 'm4v'])

    if video:
        url = 'https://coachai-ujeungn6tq-ew.a.run.app/predict'
        data = {'video': video}

        response = requests.post(url, files=data).json()

        params_list = [np.array(element) for element in response['lists']]

        params_list.append(response['label'])

        st.header(
            f"Well done! You did **{response['reps']} {response['prediction']}s**! ðŸ¥‡")

        target = exercices[response['prediction']]["Target_Muscle_Group"]

        st.subheader(f'You worked on your {target} {emojis[target]}!')

        st.subheader(f'You are one step closer to your goals! ðŸ†')

        st.sidebar.video(video)

        ##Generate the annotated video
        tfile = tempfile.NamedTemporaryFile(delete=False)
        tfile.write(video.read())
        vf = cv2.VideoCapture(tfile.name)
        frame_width = int(vf.get(3))
        frame_height = int(vf.get(4))
        fps = int(vf.get(cv2.CAP_PROP_FPS))
        inflnm, inflext = video.name.split('.')
        exercise_name = inflnm.split('_')[1]
        annotated_filename = f'{inflnm}_annotated.{inflext}'
        out_annotated = cv2.VideoWriter(annotated_filename,
                                        cv2.VideoWriter_fourcc('M', 'J', 'P', 'G'),
                                        fps, (frame_width, frame_height))

        with mp_pose.Pose(min_detection_confidence=0.5,
                        min_tracking_confidence=0.5) as pose:
            while vf.isOpened():
                ret, image = vf.read()
                if not ret:
                    break

                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
                image.flags.writeable = False
                results = pose.process(image)
                image.flags.writeable = True
                image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)

                mp_drawing.draw_landmarks(image,
                                        results.pose_landmarks,
                                        mp_pose.POSE_CONNECTIONS,
                                        landmark_drawing_spec=mp_drawing_styles.
                                        get_default_pose_landmarks_style())
                out_annotated.write(image)

            out_annotated.release()
            vf.release()
        st.subheader('')
        st.subheader('**Your body markers** ðŸ“')
        st.video(annotated_filename)
        st.subheader('')
        st.subheader('**Your body movement** ðŸ¤¸ðŸ»â€â™‚ï¸')
        fig = make_plot(params_list)

        st.write(fig)

        st.subheader('')

        st.header('**Going further...ðŸš€**')

        col1, col2 = st.columns(2)

        col1.subheader('Preparation â³')
        col1.write(exercices[response['prediction']]["Instructions_Preparation"])
        col1.text('')
        col1.subheader('Execution âœ…')
        col1.write(exercices[response['prediction']]["Instructions_Execution"])
        col1.subheader('Tips ðŸ—£')
        col1.write(exercices[response['prediction']]["Comments"])

        col2.subheader('To take it easy ðŸ')
        col2.write(exercices[response['prediction']]["Easier"])
        col2.subheader('To push harder ðŸ”¥')
        col2.write(exercices[response['prediction']]["Harder"])

        st.subheader('Good form example ðŸ‘€')

        col3, col4 = st.columns(2)

        col3.video(exercices[response['prediction']]["video_src"])

    else:
        #Image for homepage
        st.header('')
        image_intro = st.image('streamlit/intro.jpeg')

if app_mode == 'Run on Webcam':
    category = {
        0: 'Push Up',
        1: 'Jumping Jack',
        2: 'Squat',
        3: 'Lunge',
        4: 'Pull Up'
    }

    class VideoTransformer(VideoTransformerBase):
        def __init__(self):
            self.landmarks = {}
            self.i = 0
            self.landmark_dict = {}
            self.data = []
            self.prediction = ''

        def transform(self, frame):
            img = frame.to_ndarray(format="bgr24")
            frame_width = int(img.shape[1])
            frame_height = int(img.shape[0])
            j = 0
            with mp_pose.Pose(min_detection_confidence=0.5,
                            min_tracking_confidence=0.5) as pose:
                results = pose.process(img)
                mp_drawing.draw_landmarks(img,
                                        results.pose_landmarks,
                                        mp_pose.POSE_CONNECTIONS,
                                        landmark_drawing_spec=mp_drawing_styles.
                                        get_default_pose_landmarks_style())
                pose_landmarks = results.pose_landmarks
                if pose_landmarks != None:
                    for lmk in pose_landmarks.landmark:
                        self.landmarks[j] = [
                            lmk.x * frame_width, lmk.y * frame_height,
                            lmk.z * frame_width
                        ]
                        j += 1
                self.landmark_dict[self.i] = self.landmarks
                self.i += 1
                if len(self.landmark_dict) >= 20:
                    self.data.append(json_to_df(self.landmark_dict)[0])
                    print("--------self data [0]--------")
                    print(self.data[0])
                    cols = [
                        'min_d0', 'max_d0', 'min_d1', 'max_d1', 'min_d2', 'max_d2',
                        'min_d3', 'max_d3', 'min_d4', 'max_d4', 'min_d5', 'max_d5',
                        'min_d6', 'max_d6', 'min_d7', 'max_d7', 'min_d8', 'max_d8',
                        'min_d9', 'max_d9', 'min_d10', 'max_d10', 'min_d11',
                        'max_d11', 'min_d12', 'max_d12', 'min_a0', 'max_a0',
                        'min_a1', 'max_a1', 'min_a2', 'max_a2', 'min_a3', 'max_a3',
                        'min_a4', 'max_a4', 'min_a5', 'max_a5', 'mean_body_angle'
                    ]
                    X = pd.DataFrame(self.data[0])
                    X.columns = cols
                    print(X)
                    model = pickle.load(open('svc_coachai.pkl', "rb"))
                    results = model.predict(X)
                    print(results[0])
                    print(category[results[0]])
                    self.prediction = category[results[0]]
            cv2.rectangle(img, (0, 0), (225, 73), (255, 255, 255), -1)
            cv2.putText(img, f'You are doing a {self.prediction}', (15, 12), cv2.FONT_HERSHEY_SIMPLEX,
                        0.5, (0, 0, 0), 1, cv2.LINE_AA)
            return img

    webrtc_streamer(key="Start Recording", video_processor_factory=VideoTransformer)


def load_css(filename):
    with open(filename) as f:
        st.markdown("<style>{}</style>".format(f.read()), unsafe_allow_html=True)

load_css('style.css')
